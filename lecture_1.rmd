---
title: "mixed models: introduction"
author: "Ben Bolker"
date: "18 November 2025"
bibliography: "glmm.bib"
csl: apa.csl
nocite: |
  @banta_comprehensive_2010
output:
  ioslides_presentation
header-includes:
- \usepackage{amsmath}
- \usepackage{amssymbol}
- \newcommand{\X}{\boldsymbol X}
- \newcommand{\Z}{\boldsymbol Z}
- \newcommand{\bbeta}{\boldsymbol \beta}
- \newcommand{\bb}{\boldsymbol b}
- \newcommand{\bu}{\boldsymbol u}
- \newcommand{\bLambda}{\boldsymbol \Lambda}
- \newcommand{\bEta}{\boldsymbol \eta}
- \newcommand{\btheta}{\boldsymbol \theta}
- \newcommand{\bzero}{\boldsymbol 0}
---

\newcommand{\bm}[1]{#1}

<style>
.refs {
   font-size: 16px;
}
h2 { 
 color: #3399ff;		
}
h3 { 
 color: #3399ff;		
}
.title-slide {
   background-color: #55bbff;
}
</style>
<!--    content: url(https://i.creativecommons.org/l/by-sa/4.0/88x31.png)
>
<!-- Limit image width and height -->
<style type="text/css">
img {     
  max-height: 560px;     
  max-width: 800px; 
}
</style>


```{r knitr,include=FALSE, message = FALSE}
require("knitr")
knit_hooks$set(crop=hook_pdfcrop)
##opts_chunk$set(fig.width=5,fig.height=4,
##               out.width="0.6\\textwidth",
##               fig.align="center",echo=FALSE)
opts_chunk$set(echo=FALSE)
```

```{r pkgs,message=FALSE}
library(lattice)
library(plotrix) ## for axis.break
library(reshape2)
library(ggplot2)
library(RColorBrewer)
library(mvbutils) ## for foodweb()
library(lme4)
library(tidyverse)
library(MASS)
library(nlme)
library(grid)
library(ggbeeswarm)
zmargin <- theme(panel.spacing=unit(0,"lines"))
library(scales) ## for 'scientific', 'trans_format'
theme_set(theme_bw())
```

```{r setup, include=FALSE}
set.seed(20251028)
OkIt <- unname(palette.colors(n = 8, palette = "Okabe-Ito"))[-1]
scale_colour_discrete <- function(...) {
  scale_colour_manual(..., values=OkIt)
}
scale_fill_discrete <- function(...) {
  scale_fill_manual(..., values=OkIt)
}
```

## Mixed models (overview)

* *tabular* data (row = observation, column = variable)
* *regression* models
   * continuous and categorical *predictor* variables
   * single (univariate) numeric *response* variable
* with one or more *grouping variables* ('blocks', 'clusters', ...)
* for prediction or inference
* typically for 'medium-sized' data (e.g. $10^6$ rows, $10^3$ clusters, 10-20 predictors)

## Mixed models (easy version)

* take a standard linear-type model (e.g. $y_i = a + b x_i + \epsilon_i$)
* ... but observations fall in *groups*
* breaks assumption of independence (*pseudoreplication*)
* add *between-group* (random) variation:
$$
y_{ij} = a + b x_{ij} + \alpha_i + \epsilon_{ij}
$$

## single group, categorical predictor<br>(= one-way ANOVA)

```{r oneanova, echo = FALSE, fig.width = 8}
ngrp <- 8
npergrp <- 10
dd <- expand.grid(g = factor(1:ngrp), obs = 1:npergrp) |>
  arrange(g)
dd$trt <- factor(rep(0:1, each=40), labels = c("control", "treatment"))
dd$y <- simulate( ~ 1 + trt + (1|g),
                 newdata = dd,
                 newparams = list(beta = c(1, 0.5),
                                  theta = 2,
                                  sigma = 0.2))[[1]]

## subsample to get lack of balance
samp <- sample(nrow(dd), size = round(0.7*nrow(dd)), replace = FALSE)
dds <- dd[samp,]
mdat <- data.frame(x0 = c(0.5, 4.5), x1 = c(4.5, 8.5), y = c(1, 1.5),
                   trt = factor(c("control", "treatment")))


ggplot(dds, aes(g, y)) +
  geom_boxplot(aes(fill = trt), alpha = 0.2) +
  ## geom_jitter(aes(colour = trt, shape = trt),
  ##              width = 0.2, height = 0) +
  geom_beeswarm(aes(colour = trt, shape = trt)) +
  labs(x = "group", y = "response",
       title = expression(list(beta == (list(1,0.5)),
                               sigma[alpha]==0.4,
                               sigma[r] == 0.2))) +
  geom_segment(aes(x = x0, xend = x1, y = y, yend = y,
                   colour = trt), data = mdat, lty = 2,
               alpha = 0.8)
```

## The mixed model universe

* more complex *fixed effects* components (multivariate regression, multiway ANOVA, ANCOVA, ...)
* multiple grouping variables (random effects *terms*)
* multiple effects varying across groups (*random slopes models* etc.)
* non-Normal *conditional distributions*
   * exponential dispersion family (à la GLMs): binomial/Poisson/etc.
   * weirder: Tweedie, $t$ distribution, Beta, ...
* nonlinearity: *link functions* (log, logit, etc.)
* structured covariances: autoregressive, compound symmetric, regression splines ...

##

```{r out.width='100%'}
knitr::include_graphics("pix/models-glmm.png")
```

```{r culcita1,cache=TRUE,message=FALSE,warning=FALSE}
## Culcita fits
stierdat <- read.csv("data/culcitalogreg.csv")
stierdat$ttt <- as.factor(stierdat$ttt) ## treatment should be a factor
## contrast matrix
cmat <- matrix(c(0,1,1,1,0,1,-1,0,0,1,1,-2),ncol=3,
               dimnames=list(NULL,c("symb","crab.vs.shr","addsymb")))
contrasts(stierdat$ttt) <- cmat
stierdat$block <- as.factor(stierdat$block) ## not really necessary but logical
stierdat <- stierdat[,1:4]   ## don't really need the rest
mod0 <- glm(predation~ttt+block,binomial,data=stierdat)
mod1 <- glm(predation~ttt,binomial,data=stierdat)
library(lme4)
mod2 <- glmer(predation~ttt+(1|block),family=binomial,data=stierdat)
## AGQ 
mod2B <- glmer(predation~ttt+(1|block),family=binomial,
               nAGQ=20,data=stierdat)
mod5=MASS::glmmPQL(predation~ttt,random=~1|block,family=binomial,data=stierdat)
levels(stierdat$ttt) <- c("none","shrimp","crabs","both")
m <- melt(stierdat[,1:3],id.vars=1:2)
m2 <- dcast(m,ttt~variable,fun.aggregate=mean)
m3 <- dcast(m,ttt+block~variable,fun.aggregate=sum)
p <- with(m3,table(predation,ttt))
``` 


```{r culcita2,warning=FALSE,message=FALSE}
load("data/culcita.RData")
library(lme4)
cmod1 <- glmer(predation~ttt+(1|block),data=culcita_dat,
               family=binomial,nAGQ=10)
cmod3 <- glm(predation~ttt,data=culcita_dat,family=binomial)
## jump through hoops to get GLM to estimate without complete-separation blocks 7,8,9 messing everything up!
cmod4 <- suppressWarnings(glm(predation~ttt+block,data=transform(culcita_dat,block=relevel(block,2)),
             family=binomial,
             start=c(2,-4,-4,-6,-2,0,0,4,5,30,30,30,5)))
             ## contrasts=list(block=contr.sum),
             ## start=rep(0,)
             ## start=c(10,-4,-5,-6,-20,-8,-8,-8,-5,-3,20,20,20))
r <- ranef(cmod1,condVar=TRUE)
## set up plots
## FIXME:: replace with broom!
library(coefplot2)
f1 <- coeftab(cmod1)
f3 <- coeftab(cmod3)
f4 <- coeftab(cmod4)[1:4,]
r1 <- coeftab(cmod1,ptype="ranef")
r2 <- f3[1,]  ## intercept
r4 <- coeftab(cmod4)[-(1:4),]
v <- data.frame(predict(cmod4,
             newdata=data.frame(block=factor(1:10),ttt="none"),
             se.fit=TRUE))
v$fit <- v$fit-mean(v$fit)
v <- setNames(v[,1:2],c("Estimate","Std..Error"))
w <- data.frame(Estimate=rep(0,10),
                Std..Error=NA,row.names=1:10)
## less clunky way to do this?
dd <- function(X,method,type,
               strip="^ttt") {
  data.frame(method,type,
             p=gsub(strip,"",rownames(X)),X[,1:2],
             stringsAsFactors=FALSE)
}
allEsts <- rbind(dd(f1,"mixed","ttt"),
      dd(f3,"pooled","ttt"),
      dd(f4,"fixed","ttt"),
      dd(r1,"mixed","blocks",
         strip="block\\.\\(Intercept\\)"),
      dd(v,"fixed","blocks"),
      dd(w,"pooled","blocks"))
allEsts <- transform(allEsts,
   p=factor(p,levels=gtools::mixedsort(unique(p))))
```

```{r bplot1,fig.width=8,fig.height=6, eval = FALSE}
op <- par(las=1,cex=1.5,bty="l")
bb <- barplot(p,ylab="Number of blocks",xlab="Symbionts",
               main="Number of predation events")
bloc <- apply(p,2,cumsum)
midb <- rbind(bloc[1,]/2,
             (bloc[1,]+bloc[2,])/2,
             (bloc[2,]+bloc[3,])/2)
text(bb[col(p)][p>0],midb[p>0],c(0,1,2)[row(p)][p>0])
par(op)
```

## Experimental: bloodworm cell survival

```{r glycera}
x <- read.csv("data/Live-Dead Tabulated Counts.csv")
## utility function for factoring/renaming variables before
##  lattice plot
rnfac <- function(dat,vars) {
  if (!all(vars %in% names(dat))) stop("unknown variable")
  for (v in vars) {
    dat[[v]] <- factor(dat[[v]])
    levels(dat[[v]]) <- paste(v,"=",round(as.numeric(levels(dat[[v]])),2),sep="")
  }
  dat
}
sc <- function(x) { (x-min(x))/diff(range(x))}
xsc <- x
predvars <- c("Osm","Cu","H2S","Anoxia")
for (i in predvars) {
  xsc[[i]] <- sc(xsc[[i]])
}
xsc$Osm <- xsc$Osm-0.5
## xsc$O2 <- 1-xsc$O2
## names(xsc)[names(xsc)=="O2"] <- "anox"
xr0 <- within(x,FractionAlive <- Alive/(Alive+Dead))
xr <- melt(subset(xr0,select=-c(Alive,Dead)),id.vars=1:5)

x4 <- dcast(xr,H2S+Anoxia+Cu+Osm~.,fun.aggregate=mean)
names(x4)[5] <- "all"
x5 <- rnfac(x4,c("Anoxia","Osm"))

## FIXME: replace with ColorBrewer colours?
cmgen.colors  <- function (n,h1=6/12,h2=10/12,maxs=0.5)  {
    if ((n <- as.integer(n[1])) > 0) {
        even.n <- n%%2 == 0
        k <- n%/%2
        l1 <- k + 1 - even.n
        l2 <- n - k + even.n
        c(if (l1 > 0) hsv(h = h1,
                          s = seq(maxs, ifelse(even.n, 0.5/k, 0), length.out = l1),
                          v = 1),
          if (l2 > 1) hsv(h = h2,
                          s = seq(0, maxs, length.out = l2)[-1],
                          v = 1))
    }
    else character(0)
}
rb.colors <- function(n) {
  cmgen.colors(n,h1=0,h2=0.7,maxs=1)
}
```
 

```{r glycplot1,out.width="\\textwidth",fig.width=8,fig.height=5}
orig <- trellis.par.get()
pad <- 0 ## 15 for regular layout
trellis.par.set(layout.widths=list(right.padding=pad,left.padding=pad),
                regions=list(col=rb.colors(100)),
##                regions=list(col=brewer.pal(11,"RdBu")),
## leave text alone for regular layout
                add.text=list(cex=0.8),axis.text=list(cex=0.5))
levels(x5$Anoxia) <- c("Normoxia","Anoxia")
## print(levelplot(`(all)`~factor(H2S)*factor(Cu)|Anoxia*Osm,
##          col.region=rb.colors(100),
##          data=x5,
##          xlab=expression(H[2]*S),
##          ylab="Copper"))
levelplot(all~factor(H2S)*factor(Cu)|Osm*Anoxia,
                col.region=rb.colors(100), ## brewer.pal(11,"RdBu"), ## rb.colors(100),
                data=x5,
                xlab=expression(H[2]*S),
          ylab="Copper",
          sub = "D. Julian, unpublished data")
trellis.par.set(theme=orig) ## restore settings
## FIXME: redo in ggplot2?  LOW PRIORITY
```

## *Arabidopsis*: fertilization & herbivory

```{r arabplot1,fig.width=7,fig.height=6}
panel.stripplot2 <-
function (x, y, jitter.data = FALSE, factor = 0.5, amount = NULL, 
    horizontal = TRUE, groups = NULL, ...) 
{
    if (!any(is.finite(x) & is.finite(y))) 
        return()
    panel.sizeplot(x = x, y = y, jitter.x = jitter.data && !horizontal, 
        jitter.y = jitter.data && horizontal, factor = factor, 
        amount = amount, groups = groups, horizontal = horizontal, 
        ...)
}
load("data/Banta.RData")
trellis.par.set(list(fontsize=list(text=20)))
stripplot(jltf ~ amd|nutrient, 
                data=within(dat.tf,jltf <-jitter(log(total.fruits+1),
                  amount=0.05)),
                strip=strip.custom(strip.names=c(TRUE,TRUE)),
                groups=gen, type=c('p','a'),
          ylab="Log(1+fruit set)",
          sub = "Banta et al. 2010")
##                main="panel: nutrient, color: genotype")
## trellis.par.set(theme=orig) ## restore settings
```

## Coral demography

```{r coral_demog,warning=FALSE}
L <- load("data/m.acr.jagsout.RData")
L2 <- load("data/m.acr.lme4out.RData")
L3 <- load("data/demog.mort.18apr.RData")
source("R/demog_mort_funs.R")
pp <- plotfun(j.red2,m.acr.nofr,drop.cols=c(4,8))
pp + labs(subtitle = "J.-S. White, unpublished")
```

## Simple mixed models<br>(scalar, intercept-only)

$$
\begin{split}
y_{ij} & = \beta_0 + \beta_1 x_{ij} + \epsilon_{0,ij} + \epsilon_{1,j} \\
& = (\beta_0 + \epsilon_{1,j}) + \beta_1 x_{ij} + \epsilon_{1,j} \\
\epsilon_{0,ij} & \sim \textrm{Normal}(0,\sigma_0^2) \\
\epsilon_{1,j} & \sim \textrm{Normal}(0,\sigma_1^2)
\end{split}
$$

- Could have multiple, nested levels of random effects  
(genotype within population within region ...), or *crossed* REs

## Random-slopes model

$$
\begin{split}
y_{ij} & = \beta_0 + \beta_1 x_{ij} + \epsilon_{0,ij} + \epsilon_{1,j} +
\epsilon_{2,j} x_{ij} \\
& = (\beta_0 + \epsilon_{1,j}) + (\beta_1 + \epsilon_{2,j}) x_{ij})  \\
\epsilon_{0,ij} & \sim \textrm{Normal}(0,\sigma_0^2) \\
\{\epsilon_{1,j}, \epsilon_{2,j}\} & \sim \textrm{MVN}(0,\Sigma)
\end{split}
$$

- variation in the *effect* of a treatment or covariate across groups
- group intercept, slope offsets are correlated

## General definition

$$
\begin{split}
\underbrace{Y_i}_{\text{response}} & \sim \overbrace{\text{Distr}}^{\substack{\text{conditional} \\ \text{distribution}}}(\underbrace{g^{-1}(\eta_i)}_{\substack{\text{inverse} \\ \text{link} \\ \text{function}}},\underbrace{\phi}_{\substack{\text{scale} \\ \text{parameter}}}) \\
\underbrace{\boldsymbol \eta}_{\substack{\text{linear} \\ \text{predictor}}} & 
 = 
\underbrace{\boldsymbol X \boldsymbol \beta}_{\substack{\text{fixed} \\ \text{effects}}} + 
\underbrace{\boldsymbol Z \boldsymbol b}_{\substack{\text{random} \\ \text{effects}}}
\\
\underbrace{\boldsymbol b}_{\substack{\text{conditional} \\ \text{modes}}}  & 
\sim \text{MVN}(\boldsymbol 0, \underbrace{\Sigma(\boldsymbol \theta)}_{\substack{\text{variance-} \\ \text{covariance} \\ \text{matrix}}})
\end{split}
$$

## What are random effects?

A method for …

-   accounting for among-individual, within-block correlation
-   compromising between\
    *complete pooling* (no among-block variance)\
     and *fixed effects* (large among-block variance)
-   handling levels selected at random from a larger population
-   sharing information among levels (*shrinkage
    estimation*)
-   estimating variability among levels
-   allowing predictions for unmeasured levels

## Random-effect myths

-   levels of random effects must always be sampled at random
-   a complete sample cannot be treated as a random effect
-   random effects are always a *nuisance variable*
-   nothing can be said about the predictions of a random effect
-   you should always use a random effect no matter how few levels you
have

## Reasons for random effects (inferential/philosophical)

- **don't** want to
     - test hypotheses about differences between responses at particular levels of
the grouping variable;
- **do** want to
     - quantify variation among groups
     - make predictions about unobserved groups
- have levels that are randomly sampled from/representative of a larger population (*exchangeable*)


## Reasons for random effects (practical)

- want to combine information across groups
- have variation in information per level (number of samples or noisiness);
- have a categorical predictor that is a nuisance variable (i.e., it is not of direct interest, but should be controlled for).
- have more than 5-6 groups, or regularizing/using priors (otherwise, use fixed)

See also @Crawley2002; @gelman_analysis_2005

## Avoiding mixed models

* average: for *nested* LMMs (when uninterested in within-group variation) [@murtaugh_simplicity_2007] 
* use fixed effects (or two-stage models) instead of random effects when
     * many samples per block (shrinkage unimportant)
     * few blocks (little advantage; bad variance estimates)

Model setup
============

## Formulas

- random effects specified with `|` as `(a|g1) + (b|g2) + ...`
- right-hand side is the *grouping variable* (always categorical, usually a factor)
- left-hand side is the *varying term* (most often 1)s
- terms separated by `+` are independent

## Formula definitions

```{r ftab,echo=FALSE,results="asis"}
ftab <- matrix(c("β_0 + β_{1}X_{i} + e_{si}",
                 "n/a (Not a mixed-effects model)",
                 "(β_0 + b_{S,0s}) + β_{1}X_i + e_{si}",
                 "∼ X + (1∣Subject)",
                 "(β_0 + b_{S,0s}) +  (β_{1} + b_{S,1s}) X_i + e_{si}",
                 "~ X + (1 + X∣Subject)",
                 "(β_0 + b_{S,0s} + b_{I,0i}) + (β_{1} + b_{S,1s}) X_i + e_{si}",
                 "∼ X + (1 + X∣Subject) + (1∣Item)",
                 "As above, but $S_{0s}$, $S_{1s}$ independent",
                 "∼ X + (1||Subject) + (1∣Item)", 
                 "(β_0 + b_{S,0s} + b_{I,0i}) + β_{1}X_i + e_{si}",
                 "∼ X + (1∣Subject) + (1∣Item)", 
                 "(β_0 + b_{I,0i}) +  (β_{1} + b_{S,1s})X_i + e_{si}",
                 "∼ X + (0 + X∣Subject) + (1∣Item)"),
               byrow=TRUE,ncol=2,
               dimnames=list(NULL,c("equation","formula")))
ftab <- data.frame(ftab,stringsAsFactors=FALSE)
ff <- !grepl("As above",ftab$equation)
ftab$equation[ff] <- sprintf("$%s$",ftab$equation[ff])
ff <- !grepl("n/a",ftab$formula)
ftab$formula[ff] <- sprintf("`%s`",ftab$formula[ff])
pander::pander(ftab,justify="left")
```

Modified from: http://stats.stackexchange.com/questions/13166/rs-lmer-cheat-sheet?lq=1 (Livius). Subscripts $\{S,I\}$ refer to Subject vs Item effects. Lower-case $\{s,i\}$ indicate particular subjects/items. $\{0,1\}$ refer to intercept vs slope effects.

## Nesting vs crossing

Nested: sub-unit IDs only measured within a single larger unit.
e.g.: Plot1 in Block1 independent of Plot1 in Block2

![](pix/CV_nested.png)

Crossed: sub-unit IDs can be measured in multiple larger units.
e.g. year, site

![](pix/CV_crossed.png)

Unique coding: removes ambiguity

![](pix/CV_unique.png)

Robert Long, [Cross Validated](https://stats.stackexchange.com/questions/228800/crossed-vs-nested-random-effects-how-do-they-differ-and-how-are-they-specified)

## Formulas, interactions, nesting etc.

- Nested: `(1|f/g)` $\equiv$ `(1|f) + (1|f:g)`. Subplots vary within plots, no commonality across plots
- Crossed: `(1|f) + (1|g)`. Years vary; plots vary independently
- Crossed+: `(1|f*g)` $\equiv$ `(1|f) + (1|g) + (1|f:g)`. Years vary, plots vary independently, plots vary within years (need >1 observation per plot/year combination).

**Don't need explicit nesting** if sub-groups are uniquely labeled (i.e. `A1`, `A2`, ..., `B1`, `B2`, ...)

## Factors varying across groups

Can generate large variance-covariance matrices. For a four-level factor, we have $\beta_0$ (intercept), $\beta_1$ (level 2 - level 1), $\beta_2$ (level 3 - level 1), $\beta_3$ (level 4 - level 1).

$$
\Sigma = 
\left[
\begin{array}{cccc}
\sigma^2_{\{b|1\}}  & . & . & . \\
\sigma_{\{b|1\},\{b|a_{21}\}} &
\sigma^2_{\{b|a_{21}\}} & . & .  \\
\sigma_{\{b|1\},     \{b|a_{31}\}} &
\sigma_{\{b|a_{21}\},\{b|a_{31}\}} &
\sigma^2_{\{b|a_{31}\}} & . \\
\sigma_{\{b|1\}     ,\{b|a_{41}\}} &
\sigma_{\{b|a_{21}\},\{b|a_{41}\}} &
\sigma_{\{b|a_{31}\},\{b|a_{41}\}} &
\sigma^2_{\{b|a_{41}\}} 
\end{array}
\right]
$$

## What is the maximal model?

- Which effects vary *within* which groups?
- If effects don't vary within groups, then we *can't* estimate among-group variation in the effect
     - convenient
     - maybe less powerful (among-group variation is lumped into residual variation)
- e.g. female rats exposed to different doses of radiation, multiple pups per mother, multiple measurements per pup (labeled by time). Maximal model ... ?

## Maximal model is often too complex

e.g.

- *Culcita* (coral-reef) example: randomized-block design, so each treatment (none/crabs/shrimp/both) is repeated in every block; thus `(treat|block)` is maximal
- CBPP data: each herd is measured in every period, so in principle we could use `(period|herd)`, not just `(1|herd)`


Estimation
==========

## Maximum likelihood estimation

-   Best fit is a compromise between two components\
    (consistency of data with fixed effects and conditional modes;
    consistency of random effect with RE distribution)
	-   Goodness-of-fit *integrates* over conditional modes

## ...

```{r plotex,message=FALSE}
set.seed(101)
dd <- data.frame(f=gl(5,5))
dd$y <- simulate(~1+(1|f),newdata=dd,
                 family=gaussian,seed=101,
                 newparams=list(theta=1,beta=0,sigma=1))[[1]]
ggplot(dd,aes(x=f,y=y))+geom_point()+
    stat_summary(fun=mean,geom="point",size=3,colour="blue",
                 pch=3)+
     geom_point(data=subset(dd,y<(-2)),colour="red",size=2)+
         theme_update(panel.grid.major=element_blank(),
                      panel.grid.minor=element_blank())
```

## Shrinkage: *Arabidopsis* conditional modes

```{r arabshrink,fig.height=6,fig.width=8}
z<- subset(dat.tf,amd=="clipped" & nutrient=="1")
m1 <- glm(total.fruits~gen-1,data=z,family="poisson")
m2 <- glmer(total.fruits~1+(1|gen),data=z,family="poisson")
tt <- table(z$gen)
rr <- unlist(ranef(m2)$gen)[order(coef(m1))]+fixef(m2)
m1s <- sort(coef(m1))
m1s[1:2] <- rep(-5,2)
gsd <- attr(VarCorr(m2)$gen,"stddev")
gm <- fixef(m2)
nseq <- seq(-3,6,length.out=50)
sizefun <- function(x,smin=0.5,smax=3,pow=2) {
    smin+(smax-smin)*((x-min(x))/diff(range(x)))^pow
}
nv <- dnorm(nseq,mean=gm,sd=gsd)
##
op <- par(las=1,cex=1.5,bty="l")
plot(exp(m1s),xlab="Genotype",ylab="Mean fruit set",
     axes=FALSE,xlim=c(-0.5,25),log="y",yaxs="i",xpd=NA,
     pch=16,cex=0.5)
axis(side=1)
axis(side=2,at=c(exp(-5),0.1,1,10,20),
     labels=c(0,0.1,1,10,20),cex=0.8)
##     ylim=c(-3,5))
polygon(c(rep(0,50),nv*10),exp(c(rev(nseq),nseq)),col="gray",xpd=NA)
n <- tt[order(coef(m1))]
points(exp(rr),pch=16,col=adjustcolor("red",alpha=0.5),
       cex=sizefun(n),xpd=NA)
## text(seq_along(rr),rr,n,pos=3,xpd=NA,cex=0.6)
box()
axis.break(axis=2,breakpos=exp(-4))
legend("bottomright",
       c("group mean","shrinkage est."),
       pch=16,pt.cex=c(1,2),
       col=c("black",adjustcolor("red",alpha=0.5)),
       bty="n")
par(op)
```

## Shrinkage in a random-slopes model

From Christophe Lalanne, see [here](https://stats.stackexchange.com/questions/51186/what-would-be-an-illustrative-picture-for-linear-mixed-models):

```{r sleepstudy_shrinkage, echo=FALSE}
library(lme4)
data(sleepstudy)

## Fit individual regression lines for each subject
dfrm <- coef(lmList(Reaction ~ Days | Subject, sleepstudy))

## Estimate parameters of a random intercept and random intercept and slope model
m1 <- lmer(Reaction ~ Days + (1 | Subject), data=sleepstudy)
m2 <- lmer(Reaction ~ Days + (Days | Subject), data=sleepstudy)

## Put all estimates (intercept + slope for each model) into the same data.frame
dfrm <- cbind.data.frame(dfrm,
                         as.data.frame(coef(m1)[["Subject"]]),
                         as.data.frame(coef(m2)[["Subject"]]))

## Kernel density estimates for the distribution of individual intercepts
intcpt.dens <- list()
idx <- seq(1, ncol(dfrm), by=2)
for (i in seq_along(idx))
  intcpt.dens[[i]] <- density(as.numeric(dfrm[,idx[i]]), adj=1.4)
len <- length(intcpt.dens[[1]]$x)

## Show all
cols <- c("grey30", "#D95F02", "#669999")
xyplot(Reaction ~ Days, data=sleepstudy,
       xlim=c(0, 8), ylim=c(150, 450), ylab="Fitted reaction time",
       scales=list(x=list(at=seq(0, 8, by=1))),
       key=list(corner=c(0,1), text=list(c("within-group",
                                 "random intercept",
                                 "random intercept and slope"),
                                 col=cols, cex=0.8)),
       panel=function(...) {
         apply(dfrm[,1:2], 1, panel.abline, col=cols[1], alpha=.5, lwd=1.2)
         apply(dfrm[,3:4], 1, panel.abline, col=cols[2], alpha=.5, lwd=1.2)
         for (i in seq_along(idx))
         panel.lines(x=c(intcpt.dens[[i]]$y*100, rep(0, len)),
                     y=c(intcpt.dens[[i]]$x, rev(intcpt.dens[[i]]$x)), col=cols[i], lwd=1.8)
})
```

Methods
-------

## Estimation methods

- deterministic
    - various approximate integrals [@breslow_whither_2004]
    - penalized quasi-likelihood, Laplace, Gauss-Hermite quadrature, … [@biswas2015];  
best methods needed for large variance, small clusters
    -   flexibility and speed vs. accuracy
- stochastic
- stochastic (Monte Carlo): frequentist and Bayesian
    - [@booth_maximizing_1999; @sung_monte_2007; @ponciano_hierarchical_2009]
    -   usually slower but flexible and accurate

## Estimation: *Culcita* [@mckeon_multiple_2012]

```{r stierfig2,warning=FALSE,fig.width=8,fig.height=8}
library(coefplot2)
cvec <- c("purple","magenta","blue","black","darkgreen")
mnames <- c("GLM (fixed)","GLM (pooled)","PQL","Laplace","AGQ")
coefplot2(list(mod2B,mod2,mod5,mod1,
               coeftab(mod0)[1:4,]),
          col.pts=cvec,
          varnames=c("Symbiont",
            "Crab vs. Shrimp",
            "Added symbiont"),xlim=c(-7,3),
          ylim=c(0.9,3.7),
          spacing=0.14,
          main="Log-odds of predation")
par(xpd=NA)
text(rep(1,5),seq(1.6,by=-0.18,length.out=5),
     mnames,col=rev(cvec),cex=0.7,adj=0)
```

# Inference

## Wald tests

-   typical results of `summary()`
-   exact for ANOVA, regression:  
approximation for GLM(M)s
-   fast
-   approximation is sometimes awful (Hauck-Donner effect)

## Likelihood ratio tests

-   better than Wald, but still have two problems:
-   “denominator degrees of freedom”  
(when estimating scale)
    -   finite-size issues for GLM(M)s: Bartlett corrections
    -   Kenward-Roger correction? [@stroup_rethinking_2014]
-   Profile confidence intervals: expensive/fragile

## finite-size p-values??

- hope that min grps > 42
- guess denominator DF from classic design ([R code](R/calcDenDF.R))
- conservative: take minimum number of groups - 1
- Satterthwaite/Kenward-Roger (`lmerTest`, LMMs only)
- parametric bootstrap (`pbkrtest`)

## Parametric bootstrapping

-   fit null model to data
-   simulate “data” from null model
-   fit null and working model, compute likelihood difference
-   repeat to estimate null distribution
-   should be OK but ??? not well tested  
(assumes estimated parameters are "sufficiently" good)

## Bayesian inference

-   If we have a good sample from the posterior distribution (Markov chains have converged etc. etc.) we get most of the inferences we want for free by summarizing the marginal posteriors
-   *post hoc* Bayesian methods: use deterministic/frequentist methods to find the maximum, then sample around it

## resources

- <http://ms.mcmaster.ca/~bolker/misc/private/14-Fox-Chap13.pdf>
- <https://bbolker.github.io/mixedmodels-misc/ecostats_chap.html>
-  @bolker_glmm_2014

<img src="pix/foxbook.png" height="100">

(code ASPROMP8)

## references  {.refs}
