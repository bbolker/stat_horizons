---
title: "mixed models: challenges and frontiers"
author: "Ben Bolker"
bibliography: "glmm.bib"
csl: reflist2.csl
output:
  ioslides_presentation
header-includes:
- \usepackage{amsmath}
- \usepackage{amssymbol}
- \newcommand{\X}{\boldsymbol X}
- \newcommand{\Z}{\boldsymbol Z}
- \newcommand{\bbeta}{\boldsymbol \beta}
- \newcommand{\bb}{\boldsymbol b}
- \newcommand{\bu}{\boldsymbol u}
- \newcommand{\bLambda}{\boldsymbol \Lambda}
- \newcommand{\bEta}{\boldsymbol \eta}
- \newcommand{\btheta}{\boldsymbol \theta}
- \newcommand{\bzero}{\boldsymbol 0}
---

\newcommand{\bm}[1]{#1}

<style>
.refs {
   font-size: 16px;
}
h2 { 
 color: #3399ff;		
}
h3 { 
 color: #3399ff;		
}
.title-slide {
   background-color: #55bbff;
}
</style>
<!--    content: url(https://i.creativecommons.org/l/by-sa/4.0/88x31.png)
>
<!-- Limit image width and height -->
<style type="text/css">
img {     
  max-height: 560px;     
  max-width: 800px; 
}
</style>


```{r knitr,include=FALSE, message = FALSE}
require("knitr")
knit_hooks$set(crop=hook_pdfcrop)
##opts_chunk$set(fig.width=5,fig.height=4,
##               out.width="0.6\\textwidth",
##               fig.align="center",echo=FALSE)
opts_chunk$set(echo=FALSE)
```

```{r pkgs,message=FALSE}
library(lattice)
library(plotrix) ## for axis.break
library(reshape2)
library(ggplot2)
library(RColorBrewer)
library(mvbutils) ## for foodweb()
library(lme4)
library(tidyverse)
library(MASS)
library(nlme)
library(grid)
zmargin <- theme(panel.spacing=unit(0,"lines"))
library(scales) ## for 'scientific', 'trans_format'
theme_set(theme_bw())
```

```{r setup, include=FALSE}
set.seed(20251028)
OkIt <- unname(palette.colors(n = 8, palette = "Okabe-Ito"))[-1]
scale_colour_discrete <- function(...) {
  scale_colour_manual(..., values=OkIt)
}
scale_fill_discrete <- function(...) {
  scale_fill_manual(..., values=OkIt)
}
```

# Inference details

## Confidence intervals/$p$-values

* as with GLMM fitting, methods range from "easy, less accurate" to "computationally expensive, more accurate"
   * Wald approximations
   * Profile likelihood
   * (Parametric) bootstrap
* easy methods improve as number of observations *and* number of clusters grows
* easy methods work better for fixed effects than for RE variances

## Wald approximation

most important:

- small sample sizes
- values near boundaries
     - changing scales may help

more important for variance parameters than fixed-effect parameters

## Finite-size corrections

e.g. Normal vs. $t$

## Posterior predictive simulation

- if there is some value you're interested in that you can compute from your data (e.g. number of zero observations, or total NPP), you can `simulate()` it fro the fitted model:

```{r pps}

```

## Inference: fixed effects, Wald

- parameter estimates: `summary()`
- termwise tests: `car::Anova`, `afex::anova`
- contrasts/effects/post-hoc tests: `emmeans`, `effects` packages

```{r anova,results="hide",message=FALSE}
m1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy)
car::Anova(m1)
```

## Degrees of freedom

- level-counting: `R/calcDenDF.R`
- `lmerTest`/`afex`; Satterthwaite, Kenward-Roger

```{r lmerTest, message=FALSE}
library(lmerTest)
summary(as_lmerModLmerTest(m1))
summary(as_lmerModLmerTest(m1),ddf="Kenward-Roger")
```

Q: Why not use `lmerTest` all the time?  
A: it can make it a little harder to diagnose fitting problems

## Inference: fixed effects, likelihood ratio test

- individual parameters: `profile()`, `confint()`
- fit pairwise models and use `anova()` to compare
- `drop1`, `afex::anova()`

```{r ci, cache=TRUE}
confint(m1)
```

What took so long?

```{r prof,cache=TRUE,echo=FALSE,fig.width=10}
pp <- as.data.frame(profile(m1,signames=FALSE))
ggplot(pp,aes(.focal,.zeta^2))+geom_point()+geom_line()+
    facet_wrap(~.par,scale="free_x")+
    labs(x="",y="negative log-likelihood")
```

## Inference: parametric bootstrap

Very slow!

- `pbkrtest`
- `confint(.,method="boot")`

## Inference: random effects

- Wald is probably very bad
- profile, `anova()`
- parametric bootstrap
- boundary problems

## Inference: CIs on predictions etc.

(possible now. distinction between `lme4` (conditions on theta-hat) vs `glmmTMB` (full))

# Troubleshooting

## What is a "singular fit"?

* non-positive-definite covariance matrix
* zero variance (simple case)
* Â±1 correlation (next simplest case)
* hard to recognize in the general case! 
* some linear combination(s) of random-effect variables have zero variance

```{r eval = FALSE}
library(rgl)
open3d()
ellipse3d(scale = c(2, 3, 0))
cormat <- diag(3)
cormat[lower.tri(cormat)] <- c(0.6,0.2, 0.3)
cormat[upper.tri(cormat)] <- t(cormat)[upper.tri(cormat)]
v <- outer(sd, sd) * cormat
plot3d(ellipse3d(v), xlim = c(-3, 3),
       ylim = c(-3, 3), zlim = c(-3, 3), col = "blue")
lines3d(ee[,1], ee[,2], 0)
points3d(ee[,1], ee[,2], 0)
indices <- seq(nrow(ee))
wire3d( mesh3d(vertices = ee, quads = indices) )
```

## Why are fits singular?

Essentially, because the *observed* among-group variation is less than
the *expected* among-group variation (= $\sigma^2_{\mbox{among}} +
\sigma^2_{\mbox{within}}/n$). More generally, because *some* dimension
of the variance-covariance matrix has zero extent ...

```{r singsims,echo=FALSE,fig.width=7,fig.height=4,cache=TRUE,message=FALSE,warning=FALSE}
simfun <- function(n1=5,n2=5,sd1=1,sd2=1) {
  d <- expand.grid(f1=factor(seq(n1)),f2=factor(seq(n2)))
  u1 <- rnorm(n1,sd=sd1)
  d$y <- rnorm(n1*n2,mean=u1,sd=sd2)
  d
}
require(lme4)
fitfun <- function(d=simfun()) {
  sqrt(unlist(VarCorr(lmer(y~(1|f1),data=d))))
}
set.seed(101)
sd_dist1 <- replicate(500,fitfun())
sd_dist2 <- replicate(500,fitfun(simfun(n1=3)))
sd_List <- list(n1.5=sd_dist1,n1.3=sd_dist2)
plotfun <- function(x,trueval,main="") {
  par(las=1,bty="l")
  hist(x,breaks=50,col="gray",main=main,xlab="est. sd",
       freq=FALSE)
}
par(mfrow=c(1,2))
plotfun(sd_List[[1]],main="sd=1,res.sd=1,5 groups")
plotfun(sd_List[[2]],main="sd=1,res.sd=1,3 groups")
invisible(dev.off())
```

## What to do about singular fits?

* ignore them, or drop terms
* simplify the model
* regularize/add priors
* full Bayesian treatment

## Ignoring/dropping terms

* Zero variances/singular fits *are not a mistake*; they're the best fit to the data
* Dropping a (full) singular term gives the same results/predictions/etc.
    * It does narrow confidence intervals (if using profile CIs)



## Simplified versions of models

- "complex random intercepts" [@scandolaReliability2024a]
- `(1|b/a)` ([positive] compound symmetry) vs. `(a|b)`:
$$
(\textrm{intercept}, \textrm{slope}) =
\textrm{MVN}\left(\boldsymbol 0,
\left[
\begin{array}{ccccc}
\sigma^2_{\{b|1\}}  & . & . & . & . \\
\sigma_{\{b|1\},\{b|a_{21}\}} &
\sigma^2_{\{b|a_{21}\}} & . & . & . \\
\sigma_{\{b|1\},     \{b|a_{31}\}} &
\sigma_{\{b|a_{21}\},\{b|a_{31}\}} &
\sigma^2_{\{b|a_{31}\}} & . & . \\
\sigma_{\{b|1\}     ,\{b|a_{41}\}} &
\sigma_{\{b|a_{21}\},\{b|a_{41}\}} &
\sigma_{\{b|a_{31}\},\{b|a_{41}\}} &
\sigma^2_{\{b|a_{41}\}} & . \\
\sigma_{\{b|1\},\{b|a_{51}\}} &
\sigma_{\{b|a_{21}\},\{b|a_{51}\}} &
\sigma_{\{b|a_{31}\},\{b|a_{51}\}} &
\sigma_{\{b|a_{41}\},\{b|a_{51}\}} &
\sigma^2_{\{b|a_{51}\}}
\end{array}
\right]
\right)
$$
(=$(n(n+1))/2 = (4\times 5)/2 = 10$ parameters)
vs.
$$
\left[
\begin{array}{ccccc}
\sigma^2 & . & . & . & . \\
\rho \sigma^2 & \sigma^2 & . & . & . \\
\rho \sigma^2 & \rho \sigma^2 & \sigma^2 & . & .  \\
\rho \sigma^2 & \rho \sigma^2 & \rho \sigma^2 & \sigma^2 & .  \\
\rho \sigma^2 & \rho \sigma^2 & \rho \sigma^2 & \rho \sigma^2 & \sigma^2 
\end{array}
\right]
$$
where $\sigma^2 = \sigma^2_{\{b|1\}}+\sigma^2_{\{a:b|1\}}$,
$\rho = \sigma^2_{\{b|1\}}/\sigma^2$ (=2 parameters;
$\rho$ must be >0)

- `(1+x+y+z||b)`
     - independent terms
     - expands to `(1|b) + (0+x|b) + ...`
	 - `lme4` version **only works properly for continuous predictors**
	 - `afex::mixed` can do this
	 - independent model is no longer invariant to shifts/reparameterization
	 - $n$ instead of $n(n+1)/2$ parameters
- RE "nested within" FE, e.g. if not enough groups at the top level;
`y ~ g1 + (1|g1:g2)`; *more* parameters but fixed rather than random

## Convergence failures ![](pix/skullcross_tiny.png)

- convergence failures are common
- what do they really mean? how to fix them? when can they be ignored?
- **approximate** test that gradient=0 and curvature is correct
- scale and center predictors; simplify model
- use `?allFit` to see whether different optimizers give sufficiently similar answers
     - `$fixef`, etc.: are answers sufficiently similar?
     - `$llik`: how similar is goodness-of-fit?

```{r allFit_ex, cache=TRUE, message=FALSE}
m1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy)
aa <- allFit(m1)
ss <- summary(aa)
names(ss)
ss$fixef
```

```{r allFit_more}
ss$sdcor
ss$llik-min(ss$llik)
```

## How to decide what model to use?

- Most complex RE model that can reasonably be fitted
- Lots of disagreement
     - @barr_random_2013: "keep it maximal"
     - @bates_parsimonious_2015, @matuschek_balancing_2017: more initial parsimony
	 - ? use most complex non-singular model ?

PCA of RE variance-covariance matrix:

```{r rePCA}
rePCA(m1)
```


# Structured covariances

##

```{r}
knitr::include_graphics("pix/ab1mvs.jpg")
```

# Challenges & open questions

## On beyond `lme4`

-  `glmmTMB`: zero-inflated and other distributions
-  `brms`,`rstanarm`: interfaces to Stan
-  `INLA`: spatial and temporal correlations
- [rethinking package](https://github.com/rmcelreath/rethinking)

## Toolboxes

- JAGS (R: `rjags`, `r2jags`)
- TensorFlow (R: `greta`)
- NIMBLE (R: `nimble` package)
- Stan (R: `rstan`)
- TMB (R: `TMB`)

## On beyond R

-   Julia: `MixedModels.jl` package
-   SAS: PROC MIXED, NLMIXED
-   AS-REML
-   Stata (GLLAMM, xtmelogit)
-   HLM, MLWiN

![image](pix/300px-On_Beyond_Zebra.jpg){width="3cm"}

## Challenges

-   Small clusters: need AGQ/MCMC
-   Small numbers of clusters: need finite-size corrections (KR/PB/MCMC)
-   Small data sets: issues with *singular* fits  
[@barr_random_2013] vs. [@bates_parsimonious_2015]
-   Big data: speed!
-   Model diagnosis
-   Confidence intervals accounting for uncertainty in variances

## The need for speed [@brooks_modeling_2017]

<!-- https://tex.stackexchange.com/questions/250623/how-to-generate-a-4-up-table-of-images-with-pandoc -->


![](pix/bigfit2.png)

## more speed

![](pix/bigfit1.png)
 

## Spatial and temporal correlations

-   Sometimes blocking takes care of non-independence ...
-   but sometimes there is temporal or spatial correlation *within* blocks
-    ... also phylogenetic ... [@ives_statistics_2006]
-   "G-side" vs. "R-side" effects
-   tricky to implement for GLMMs, but new possibilities on the horizon
[@Rue+2009; @rousset_testing_2014];
[https://github.com/stevencarlislewalker/lme4ord](lme4ord package)

## Next steps

-   Complex random effects:   
    regularization, model selection, penalized methods (lasso/fence)
-   Flexible correlation and variance structures
-   Flexible/nonparametric random effects distributions
-   hybrid & improved MCMC methods
-   *Reliable* assessment of out-of-sample performance

## references  {.refs}
